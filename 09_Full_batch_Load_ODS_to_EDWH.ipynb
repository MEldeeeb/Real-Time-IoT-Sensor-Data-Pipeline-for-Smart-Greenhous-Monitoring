{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95dbbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "%run 05_utils.ipynb\n",
    "%run 01_Credentials.ipynb\n",
    "import os \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec13688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Billing_DWH..... \n",
      "Connection to PostgerSQL is done successfuly\n",
      "conn and cur are created and successfuly returned\n"
     ]
    }
   ],
   "source": [
    "conn,cur = connect_to_db(\"GH_Sensors_DWH\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3493c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # read data from dwh tables convert it into pandas data frame  \n",
    "    df_fa_measur = pd.read_sql(\"SELECT * FROM sensor_measurements\", conn)\n",
    "    df_dim_sensor = pd.read_sql(\"SELECT * FROM sensor_dimension\", conn)\n",
    "    df_dim_loc = pd.read_sql(\"SELECT * FROM location_dimension\", conn)\n",
    "    df_dim_date = pd.read_sql(\"SELECT * FROM time_dimension\", conn)\n",
    "\n",
    "    # save pandas data frames into csv files \n",
    "    df_fa_measur.to_csv(\"D:\\ITI matrial\\Week_12_DWH\\GH_Real_time_Data_Streaming_Pipeline\\CSV_data\\measurements.csv\", index=False)\n",
    "    df_dim_sensor.to_csv(\"D:\\ITI matrial\\Week_12_DWH\\GH_Real_time_Data_Streaming_Pipeline\\CSV_data\\dim_sensor.csv\", index=False)\n",
    "    df_dim_loc.to_csv(\"D:\\ITI matrial\\Week_12_DWH\\GH_Real_time_Data_Streaming_Pipeline\\CSV_data\\dim_location.csv\", index=False)\n",
    "    df_dim_date.to_csv(\"D:\\ITI matrial\\Week_12_DWH\\GH_Real_time_Data_Streaming_Pipeline\\CSV_data\\dim_date.csv\", index=False)\n",
    "except ValueError:\n",
    "    print('there is an error occared')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "096dcab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the existing data after exporting existent data from it to csv files to make it clear from the next day transaction\n",
    "# cur.execute(\"\"\"TRUNCATE TABLE sensor_measurements\"\"\")\n",
    "# cur.execute(\"\"\"TRUNCATE TABLE time_dimension\"\"\")\n",
    "# cur.execute(\"\"\"TRUNCATE TABLE location_dimension\"\"\")\n",
    "# cur.execute(\"\"\"TRUNCATE TABLE sensor_dimension\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d881513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake connection parameters\n",
    "conn = snowflake.connector.connect(\n",
    "    user='MEldeeb',\n",
    "    password= os.environ.get('snowflake_pass'),\n",
    "    account='VMFPMHB-OR45087', \n",
    "    warehouse='COMPUTE_WH',\n",
    "    database='GH_DWH',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d11e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x219ac7e9700>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a stage in Snowflake to hold the CSV files read from the ODS\n",
    "\n",
    "#load dim_location.csv into GH_DATA_STAGE data stage\n",
    "cur.execute(r\"\"\"\n",
    "    PUT 'file://D:/ITI matrial/Week_12_DWH/GH_Real_time_Data_Streaming_Pipeline/CSV_data/dim_location.csv' \n",
    "    @GH_DATA_STAGE \n",
    "    AUTO_COMPRESS=TRUE\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "#load dim_date.csv into GH_DATA_STAGE data stage\n",
    "cur.execute(r\"\"\"\n",
    "    PUT 'file://D:/ITI matrial/Week_12_DWH/GH_Real_time_Data_Streaming_Pipeline/CSV_data/dim_date.csv' \n",
    "    @GH_DATA_STAGE \n",
    "    AUTO_COMPRESS=TRUE\n",
    "\"\"\")\n",
    "\n",
    "#load dim_sensor.csv into GH_DATA_STAGE data stage\n",
    "cur.execute(r\"\"\"\n",
    "    PUT 'file://D:/ITI matrial/Week_12_DWH/GH_Real_time_Data_Streaming_Pipeline/CSV_data/dim_sensor.csv' \n",
    "    @GH_DATA_STAGE \n",
    "    AUTO_COMPRESS=TRUE\n",
    "\"\"\")\n",
    "\n",
    "#load fact_measurements.csv into GH_DATA_STAGE data stage\n",
    "cur.execute(r\"\"\"\n",
    "    PUT 'file://D:/ITI matrial/Week_12_DWH/GH_Real_time_Data_Streaming_Pipeline/CSV_data/measurements.csv' \n",
    "    @GH_DATA_STAGE \n",
    "    AUTO_COMPRESS=TRUE\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f04b577b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x219ac7e9700>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate the existing tables before loading new data \n",
    "# we use this in the first time only\n",
    "cur.execute(\"\"\"TRUNCATE TABLE FACT_MEASUREMENTS\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE TABLE DIM_DATE\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE TABLE DIM_LOCATION\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE TABLE DIM_SENSOR\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f0a9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x219ac7e9700>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy data from the stage to the Snowflake tables \n",
    "\n",
    "# Copy data into DIM_LOCATION table from the staged file (dim_date.csv.gz)\n",
    "cur.execute(\"\"\"\n",
    "    COPY INTO DIM_DATE FROM @GH_DATA_STAGE/dim_date.csv.gz\n",
    "    FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "\"\"\")\n",
    "\n",
    "# Copy data into DIM_SENSOR table from the staged file (dim_sensor.csv.gz)\n",
    "cur.execute(\"\"\"\n",
    "    COPY INTO DIM_LOCATION FROM @GH_DATA_STAGE/dim_location.csv.gz\n",
    "    FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Copy data into DIM_LOCATION table from the staged file (dim_location.csv.gz)\n",
    "cur.execute(\"\"\"\n",
    "    COPY INTO DIM_SENSOR FROM @GH_DATA_STAGE/dim_sensor.csv.gz\n",
    "    FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "\"\"\")\n",
    "\n",
    "# Copy data into FACT_MEASUREMENTS table from the staged file (measurements.csv.gz)\n",
    "cur.execute(\"\"\"\n",
    "    COPY INTO FACT_MEASUREMENTS FROM @GH_DATA_STAGE/measurements.csv.gz\n",
    "    FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4884e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
